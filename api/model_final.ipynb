{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "df_model = pd.read_csv('/home/utilisateur/Documents/dev_ia/US_SBA/loan_project/SBAnational.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model = df_model.drop(['Name','City','Bank','CreateJob','ApprovalDate','RetainedJob','ChgOffDate', 'DisbursementDate','DisbursementGross','BalanceGross','ChgOffPrinGr'],axis=1)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des colonnes ApprovalFY GrAppv SBA_Appv LowDoc MIS_Status FranchiseCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimine les erreurs qui ne sont pas des INT du type '1997A'\n",
    "df_model['ApprovalFY'] = pd.to_numeric(df_model['ApprovalFY'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Retirer les dollars\n",
    "df_model['GrAppv'] = pd.to_numeric(df_model['GrAppv'].str.replace('[\\$,]', '', regex=True))\n",
    "df_model['SBA_Appv'] = pd.to_numeric(df_model['SBA_Appv'].str.replace('[\\$,]', '', regex=True))\n",
    "\n",
    "\n",
    "# Remplace les valeurs étranges de 'RevLineCr' et 'LowDoc' par des Y et N\n",
    "df_model['RevLineCr'] = df_model['RevLineCr'].replace(('T','`',',','C','3','2','R','7','A','5','.','4','-','Q'), 'IdK')\n",
    "df_model['RevLineCr'] = df_model['RevLineCr'].replace(('0','1'), ('N','Y'))\n",
    "\n",
    "# Si le montant du prêt est inférieur à 150 000$, il s'agit d'un LowDoc\n",
    "condition_lowdoc = (df_model['GrAppv'] > 150000)\n",
    "df_model.loc[condition_lowdoc, 'LowDoc'] = 'N'\n",
    "df_model.loc[~condition_lowdoc, 'LowDoc'] = 'Y'\n",
    "\n",
    "# Encode la target en binaire \n",
    "df_model['MIS_Status'] = df_model['MIS_Status'].replace(('CHGOFF','P I F'), ('BAD','GOOD'))\n",
    "\n",
    "# Encode la colonne 'FranchiseCode' en binaire, afin de séparer les franchisés et les non franchisés\n",
    "condition_franchise = (df_model['FranchiseCode'] > 1)\n",
    "df_model.loc[condition_franchise, 'FranchiseCode'] = 1\n",
    "df_model.loc[~condition_franchise, 'FranchiseCode'] = 0\n",
    "\n",
    "\n",
    "# Imputation\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_model[['LowDoc']] = imputer.fit_transform(df_model[['LowDoc']])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_model[['ApprovalFY']] = imputer.fit_transform(df_model[['ApprovalFY']])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_model[['State','BankState']] = imputer.fit_transform(df_model[['State','BankState']])\n",
    "df_model.isnull().sum()\n",
    "\n",
    "df_model['ApprovalFY'] = df_model['ApprovalFY'].astype(int)\n",
    "df_model['GrAppv'] = df_model['GrAppv'].astype(int)\n",
    "df_model['SBA_Appv'] = df_model['SBA_Appv'].astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de State et ZIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage de Farid\n",
    "\n",
    "# nettoyage de la colonne State\n",
    "\n",
    "missin_state_rows = df_model[df_model['State'].isnull()]\n",
    "print(missin_state_rows[['Zip', 'State']])\n",
    "print()\n",
    "\n",
    "zip_values = missin_state_rows['Zip'].tolist()\n",
    "\n",
    "print()\n",
    "state_values_for_zip = []\n",
    "for zip_value in zip_values:\n",
    "    city_values_for_zip = df_model[df_model['Zip'] == zip_value]['State'].unique()\n",
    "    state_values_for_zip.append(city_values_for_zip)\n",
    "\n",
    "for i, zip_value in enumerate(zip_values):\n",
    "    print(f\"pour le {zip_value},  {state_values_for_zip[i]}\")\n",
    "print()\n",
    "\n",
    "\n",
    "zip_state_pairs = [(8070,'NJ'),\n",
    "                   (95682,'CA'),\n",
    "                   (67219,'KS'),\n",
    "                   (79925,'TX'),\n",
    "                   (33410,'FL'),\n",
    "                   (54205,'WI'),\n",
    "                   (54025, 'MN'),\n",
    "                   (84124,'UT'),\n",
    "                   (65049,'MO'),\n",
    "                   (75236,'TX'),\n",
    "                   (76052,'TX'),\n",
    "                   (76645,'TX'),\n",
    "\n",
    "]\n",
    "\n",
    "for zip_value, state_value in zip_state_pairs:\n",
    "    df_model.loc[(df_model['Zip']== zip_value) & (df_model['State'].isnull()), 'State'] = state_value\n",
    "\n",
    "\n",
    "# print(city_values_for_zip)\n",
    "\n",
    "print()\n",
    "\n",
    "# Traitement quand Zip == 0\n",
    "\n",
    "# Remplacer les valeurs 0 par NaN pour faciliter le traitement\n",
    "df_model['Zip'] = df_model['Zip'].replace(0, pd.NA)\n",
    "\n",
    "# State quand Zip is Nan \n",
    "state_values_with_zip_0 = df_model.query('Zip.isna()')['State'].unique()\n",
    "print(state_values_with_zip_0)\n",
    "\n",
    "# Boucle pour itérer sur chaque état dans la liste\n",
    "for state in state_values_with_zip_0:\n",
    "    # Utiliser query pour filtrer les lignes correspondantes à l'état actuel et extraire les codes postaux uniques\n",
    "    unique_zips_for_state = df_model.query('State == @state')['Zip'].unique()\n",
    "    \n",
    "    # Afficher l'état et les codes postaux correspondants\n",
    "    print(f\"État : {state}, Codes postaux uniques : {unique_zips_for_state}\")\n",
    "\n",
    "# Déterminer le code postal le plus fréquent pour chaque état\n",
    "most_frequent_zip = df_model.groupby('State')['Zip'].transform(lambda x: x.mode()[0])\n",
    "\n",
    "# Remplacer les valeurs NaN par le code postal le plus fréquent\n",
    "df_model['Zip'].fillna(most_frequent_zip, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyage Bank \n",
    "\n",
    "# df_model['Bank'].head(1000)\n",
    "\n",
    "# bank_state_counts = df_model.groupby('Bank')['BankState'].nunique()\n",
    "# classement_bank = bank_state_counts.sort_values(ascending=False)\n",
    "# print(classement_bank)\n",
    "\n",
    "# correlation\n",
    "\n",
    "df_model['BankState'].unique()\n",
    "contingency_table = pd.crosstab(df_model['BankState'], df_model['State'])\n",
    "print(contingency_table)\n",
    "\n",
    "# chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# print(p_value)\n",
    "\n",
    "# Qd BankState est Nan, connaitre les valeurs de State\n",
    "\n",
    "states_with_nan_bankstate = df_model.query('BankState.isnull()')['State'].unique()\n",
    "print(states_with_nan_bankstate)\n",
    "\n",
    "# determiné la valeur de Bankstate la plus fréquente lorsque State contient une valeur de states_with_nan_bankstate\n",
    "\n",
    "most_common_bank_states = {}\n",
    "\n",
    "for state in states_with_nan_bankstate:\n",
    "    filtered_df = df_model[df_model['State']== state]\n",
    "    most_common_bank_state = filtered_df['BankState'].mode()\n",
    "    most_common_bank_states[state] = most_common_bank_state\n",
    "\n",
    "for state, bank_state in most_common_bank_states.items():\n",
    "    print(f\"Pour l'état {state} la valeur la plus fréquente de BankState est {bank_state}\")\n",
    "    df_model.loc[(df_model['State'] == state) & (df_model['BankState'].isnull()), 'BankState'] = bank_state.values[0]\n",
    "\n",
    "# # Déterminer l'etat de le state le plus fréquent pour chaque bank state \n",
    "# most_frequent_state = df_model.groupby('State')['BankState'].transform(lambda x: x.mode()[0])\n",
    "# print(most_frequent_state)\n",
    "\n",
    "\n",
    "# # Remplacer les valeurs NaN par le code postal le plus fréquent\n",
    "# df_model['Zip'].fillna(most_frequent_zip, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage de NoEmp et NAICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une colonne pour la catégorie d'entreprise en fonction du nombre d'employés\n",
    "df_model['Entreprise_category'] = pd.cut(df_model['NoEmp'], \n",
    "                                          bins=[-np.inf, 10, 250, 5000, np.inf], \n",
    "                                          labels=['TPE', 'PME', 'ETI', 'TGE'], \n",
    "                                          right=False)\n",
    "\n",
    "# Convertir la catégorie en chaîne de caractères pour la lisibilité\n",
    "df_model['Entreprise_category'] = df_model['Entreprise_category'].astype(str)\n",
    "\n",
    "# # On drop la colonne NoEmp\n",
    "df_model = df_model.drop('NoEmp', axis=1)\n",
    "# Dictionnaire de correspondance des codes NAICS aux catégories\n",
    "df_model['NAICS_Category'] = df_model['NAICS'].astype(str).str[:2]\n",
    "df_model['NAICS_Category'] = df_model['NAICS_Category'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "naics_categories = {\n",
    "    '0' : 'Inconnue',\n",
    "    '11': 'Agriculture, Forestry, Fishing and Hunting',\n",
    "    '21': 'Mining, Quarrying, and Oil and Gas Extraction',\n",
    "    '22': 'Utilities',\n",
    "    '23': 'Construction',\n",
    "    '31': 'Manufacturing',\n",
    "    '32': 'Manufacturing',\n",
    "    '33': 'Manufacturing',\n",
    "    '42': 'Wholesale Trade',\n",
    "    '44': 'Retail Trade',\n",
    "    '45': 'Retail Trade',\n",
    "    '48': 'Transportation and Warehousing',\n",
    "    '49': 'Transportation and Warehousing',\n",
    "    '51': 'Information',\n",
    "    '52': 'Finance and Insurance',\n",
    "    '53': 'Real Estate and Rental and Leasing',\n",
    "    '54': 'Professional, Scientific, and Technical Services',\n",
    "    '55': 'Management of Companies and Enterprises',\n",
    "    '56': 'Administrative and Support and Waste Management and Remediation Services',\n",
    "    '61': 'Educational Services',\n",
    "    '62': 'Health Care and Social Assistance',\n",
    "    '71': 'Arts, Entertainment, and Recreation',\n",
    "    '72': 'Accommodation and Food Services',\n",
    "    '81': 'Other Services (except Public Administration)',\n",
    "    '92': 'Public Administration'\n",
    "}\n",
    "\n",
    "# Appliquer la correspondance à la colonne contenant les deux premiers chiffres des codes NAICS\n",
    "df_model['NAICS_Category'] = df_model['NAICS'].astype(str).str[:2].map(naics_categories)\n",
    "\n",
    "# On drop la colonne NAICS\n",
    "df_model = df_model.drop('NAICS', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y a t il des 0 dans le df ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colonne_avec_zero = ['LoanNr_ChkDgt','State','Zip','BankState','ApprovalFY','Term','NewExist','FranchiseCode','UrbanRural','RevLineCr','LowDoc','GrAppv','SBA_Appv','NAICS_Category']\n",
    "\n",
    "for i in colonne_avec_zero:\n",
    "    nombre_de_zeros = (df_model[i] == 0).sum()\n",
    "    print(f\"Nombre de zéros dans la colonne {i}:\", nombre_de_zeros)\n",
    "colonne_avec_zero = ['LoanNr_ChkDgt','State','Zip','BankState','ApprovalFY','Term','NewExist','FranchiseCode','UrbanRural','RevLineCr','LowDoc','GrAppv','SBA_Appv','NAICS_Category']\n",
    "\n",
    "for i in colonne_avec_zero:\n",
    "    nombre_de_zeros = (df_model[i] == 0).sum()\n",
    "    print(f\"Nombre de zéros dans la colonne {i}:\", nombre_de_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop les valeurs manquantes du df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model = df_model.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nettoyage de NewExist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Adapter l'imputeur aux données avec les valeurs différentes de 0.0\n",
    "imputer.fit(df_model[df_model['NewExist'] != 0.0][['NewExist']])\n",
    "\n",
    "# Remplacer les valeurs 0.0 par la valeur la plus fréquente\n",
    "df_model['NewExist'] = imputer.transform(df_model[['NewExist']]).ravel()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Remplacez les zéros par NaN dans la colonne \"Term\"\n",
    "df_model['Term'] = df_model['Term'].replace(0, np.nan)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Créer un objet IterativeImputer\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "# Sélectionner les valeurs à imputer (valeurs NaN dans la colonne \"Term\")\n",
    "X_impute = df_model[['Term']]\n",
    "\n",
    "# Imputer les valeurs NaN\n",
    "X_imputed = imputer.fit_transform(X_impute)\n",
    "\n",
    "# Mettre à jour la colonne \"Term\" avec les valeurs imputées\n",
    "df_model['Term'] = X_imputed\n",
    "\n",
    "# Vérifier à nouveau les valeurs uniques pour confirmer les modifications\n",
    "print(df_model['Term'].unique())\n",
    "\n",
    "df_model['Term'] = df_model['Term'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State         object\n",
       "Zip            int64\n",
       "BankState     object\n",
       "ApprovalFY     int64\n",
       "Term           int64\n",
       "MIS_Status    object\n",
       "GrAppv         int64\n",
       "SBA_Appv       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colonnes_supp = ['LoanNr_ChkDgt','NewExist','FranchiseCode','UrbanRural','RevLineCr','LowDoc','Entreprise_category','NAICS_Category']\n",
    "df_model = df_model.drop(colonnes_supp, axis=1)\n",
    "df_model.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47711, 46526, 47401, ..., 70036, 66549, 26134])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['Zip'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['State'] = df_model['State'].astype(str)\n",
    "df_model['BankState'] = df_model['BankState'].astype(str)\n",
    "df_model['Zip'] = df_model['Zip'].astype(int)\n",
    "df_model['ApprovalFY'] = df_model['ApprovalFY'].astype(int)\n",
    "df_model['Term'] = df_model['Term'].astype(int)\n",
    "df_model['GrAppv'] = df_model['GrAppv'].astype(int)\n",
    "df_model['SBA_Appv'] = df_model['SBA_Appv'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State         object\n",
       "Zip            int64\n",
       "BankState     object\n",
       "ApprovalFY     int64\n",
       "Term           int64\n",
       "MIS_Status    object\n",
       "GrAppv         int64\n",
       "SBA_Appv       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modelisation catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "X = df_model.drop(['MIS_Status'], axis=1)\n",
    "y = df_model.MIS_Status\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.95, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "num_col = list(X.select_dtypes(include=[float,int]).columns)\n",
    "cat_col = list(X.select_dtypes(include=[object]).columns)\n",
    "\n",
    "onehotscale_pipeline = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "scale_pipeline = make_pipeline(RobustScaler(with_centering=False))\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('categorical', onehotscale_pipeline, cat_col),\n",
    "        ('numerical', scale_pipeline, num_col)]\n",
    ")\n",
    "\n",
    "\n",
    "my_final_pipeline = make_pipeline(preprocessing)\n",
    "my_final_pipeline.fit(X_train)\n",
    "\n",
    "feature_names = my_final_pipeline.get_feature_names_out(X.columns)\n",
    "\n",
    "model = make_pipeline(\n",
    "    my_final_pipeline,\n",
    "    # RandomForestClassifier(random_state=42,max_depth=50)\n",
    "    CatBoostClassifier(random_state=42, depth=6,verbose=False)\n",
    ")\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "# Définissez les paramètres de la grille que vous souhaitez rechercher\n",
    "param_grid = {\n",
    "    'catboostclassifier__n_estimators': [100, 200, 300],\n",
    "    'catboostclassifier__depth': [4, 6, 8]   \n",
    "}\n",
    "\n",
    "# Créez l'objet GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit du modèle sur les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Affichage des meilleurs paramètres et de la meilleure précision\n",
    "print(\"Meilleurs paramètres trouvés :\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Prédiction sur les données de test avec le meilleur modèle\n",
    "y_pred = grid_search.predict(X_test) \n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report)\n",
    "\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# display(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # Obtenez l'importance des caractéristiques à partir de l'attribut feature_importances_\n",
    "# feature_importances = model.named_steps['catboostclassifier'].feature_importances_\n",
    "\n",
    "# # Créer une DataFrame pour associer les caractéristiques à leurs importances\n",
    "# importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# # Trier les caractéristiques par ordre d'importance\n",
    "# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Afficher les caractéristiques les plus importantes\n",
    "# print(importance_df.head(10))  # Par exemple, les 10 premières caractéristiques les plus importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvergarde et chargement Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "\n",
    "# # Sauvegarder le modèle dans un fichier\n",
    "# with open('modele.pkl', 'wb') as fichier:\n",
    "#     pickle.dump(model, fichier)\n",
    "\n",
    "# with open('modele.pkl', 'rb') as fichier:\n",
    "#     modele_charge = pickle.load(fichier)\n",
    "\n",
    "# def prediction(modele_charge, data):\n",
    "#     predictions = modele_charge.predict(data)\n",
    "#     return predictions\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "model_path = 'model.pkl'\n",
    "dump(model, model_path)\n",
    "model = load('model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9529646394478555"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de la fonction Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GOOD'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_prod(model,data):\n",
    "    # Convertir la liste de listes en DataFrame\n",
    "    df = pd.DataFrame(data, columns=['State', 'Zip', 'BankState', 'ApprovalFY', 'Term', 'GrAppv', 'SBA_Appv'])\n",
    "    predictions = model.predict(df)\n",
    "    return predictions\n",
    "\n",
    "predict_prod(model,[[\"IN\", 47711, \"OH\", 1997, 84, 60000, 48000]])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
